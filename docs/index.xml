<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>数値実験部屋</title>
    <link>https://kitano-AppMath.github.io/kitano_blog/</link>
    <description>Recent content on 数値実験部屋</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Sat, 18 Feb 2023 00:21:07 +0900</lastBuildDate><atom:link href="https://kitano-AppMath.github.io/kitano_blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gershgorinの定理</title>
      <link>https://kitano-AppMath.github.io/kitano_blog/posts/2023-02-18/</link>
      <pubDate>Sat, 18 Feb 2023 00:21:07 +0900</pubDate>
      
      <guid>https://kitano-AppMath.github.io/kitano_blog/posts/2023-02-18/</guid>
      <description>目次 定理の概要と例 概要 定理の内容 例１：単純な例 例２：相似変換 例３：狭義優対角行列 定理の概要と例 概要 Gershgorinの定理1は，行列の固有値のおおよその存在範囲を教えてくれる，ありがたい定理である．この定理だけで一冊の本2になるほどご利益がある．応用としては，数値解析をはじめ，グラフ理論などにも利用される．主張自体は数値解析の書籍3に書いてあることが多いが，ここではN. Highamのブログ記事4を参考にして，いくつか例を調べてみよう．実験に使ったJuliaプログラムはこちら．
定理の内容 定理のstatementは以下の通りだ．
定理（Gershgorinの定理） 複素正方行列$A=(a_{ij})\in\mathbb{C}^{n\times n}$と，その任意の固有値$\lambda$に対して，次式が成り立つ： \begin{equation} \lambda \in \bigcup_{i=1}^{n}\left\{ z\in\mathbb{C} \mid |z-a_{ii}| \leq \sum_{j\neq i}{|a_{ij}|} \right\}. \end{equation} つまり，任意の固有値の存在範囲が閉円盤の和集合に限定される ことを主張している．しかも存在範囲は行列の成分からすぐに分かる．ここで注意点だが，固有値を含む円盤が存在することを主張しているだけで，当然，固有値を含まない円盤が存在する可能性もある ．しかし，実は，上の定理の精密化5が存在して，固有値を含まない円盤が他の円盤と独立して存在する可能性は排除される．つまり，固有値を含まない円盤は，固有値を含む他の円盤と必ず繋がっている．まあ，とりあえず例を見てみよう．
例１：単純な例 適当に行列を作って試してみよう．今回使った行列は \begin{equation} A_1 = \begin{bmatrix} 1&amp;0&amp;1+i&amp;1&amp;0\\ -1&amp;-1&amp;0&amp;-1&amp;0\\ 2&amp;1&amp;1&amp;-1&amp;0\\ 0&amp;2&amp;1&amp;1&amp;0\\ 2&amp;0&amp;1&amp;0&amp;1 \end{bmatrix}. \end{equation} 下図の赤い円が円盤の境界，黒点が固有値である．
例２：相似変換 行列の相似変換により固有値は変わらないが，当然，円盤は変わる．Gershgorinの定理の観点から，対角化などの相似変換について考えてみよう．行列の対角化ができるならば，非対角成分が零になるから，円盤の半径も零になる．つまり，円盤は固有値の点に潰れるわけだ．Gershgorinの定理が固有値の存在範囲に関する情報を与えることを考えると，対角化のようなコストのかかる変換により固有値の存在範囲が絞られるのは自然なことである．同様に考えれば，仮に対角化まではできなくても，対角化に準ずる変換により固有値の存在範囲を絞る ことができると考えられる．ここでは上三角化を考えてみよう．
次のような行列を考えよう： \begin{equation} A_2 = \begin{bmatrix} -1&amp;1&amp;0\\ 1&amp;3&amp;1\\ 1&amp;-3&amp;0 \end{bmatrix}. \end{equation} このとき，正則行列 \begin{equation} P_2 = \begin{bmatrix} 1&amp;0&amp;0\\ 0&amp;1&amp;0\\ -1&amp;-2&amp;1\end{bmatrix} \end{equation} により， \begin{equation} \Lambda_2 = P_2^{-1}A_2P_2 = \begin{bmatrix} -1&amp;1&amp;0\\0&amp;1&amp;1\\0&amp;0&amp;2\end{bmatrix} \end{equation} と上三角化できる．円盤が小さくなったか確認してみよう．左下図は元の行列$A_2$，右下図はその上三角化$\Lambda_2$である．確かに固有値の存在範囲がより狭くなっている．</description>
    </item>
    
    <item>
      <title>Arnoldの猫写像</title>
      <link>https://kitano-AppMath.github.io/kitano_blog/posts/2023-02-14/</link>
      <pubDate>Tue, 14 Feb 2023 11:12:15 +0900</pubDate>
      
      <guid>https://kitano-AppMath.github.io/kitano_blog/posts/2023-02-14/</guid>
      <description>目次 定義とお絵描き 概要 定義 サメの絵で実験 定義とお絵描き Arnoldの猫写像を定義して，かる〜く実験する．画像さえあれば実験できるので，やってみてね．
概要 Arnoldの猫写像とは，Arnoldが作った，2次元から2次元への&amp;quot;chaotic&amp;quot;な写像である．そして入力の画像にはなぜか猫のイラストが使われる．応用としては，カオスベースの画像暗号化技術などが挙げられる1．後ほど見るように，この写像には周期がある．暗号化など，セキュリティ分野への応用を考えれば，周期の研究に関心が向くのは自然だろう．どうやらDyson and Folk2の研究が代表的らしい．Bao and Yang1でも周期に関する公式が導出されている．Fibonacci数列との関連も興味深いが，この記事ではネット上の講義資料3を参考にいくつか実験してみよう．実験に使ったJuliaプログラムはこちら．
定義 定義（Arnoldの猫写像） 自然数$N$に対して，Arnoldの猫写像とは，次式で定まる写像$F\colon\mathbb{R}^2\to\mathbb{R}^2$を指す： \begin{equation} F\left(\begin{bmatrix} x \\ y \end{bmatrix}\right) = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} \quad \pmod N. \end{equation} また，冒頭に述べた周期も定義しておこう．
定義 （周期） 自然数$N$とArnoldの猫写像$F$に対して， \begin{equation} T = \min\left\{ k\in\mathbb{N} \mid \text{任意の点$p$に対して，}F^k(p) = p \right\} \end{equation} を周期という．
サメの絵で実験 次のような画像で実験してみよう．
この画像に猫写像を何度か作用させるのだ．一旦はぐちゃぐちゃになるんだが，何度か作用させると元に戻る．このとき，作用させた回数が周期だ．まあ，とりあえずやってみよう．$15$回ほど反復させた図を下図に示した．
画像サイズ$N$に対して周期がどのように変化するか調べてみよう．周期に関してはいくつか式が知られているが，ここではコンピュータの力を借りて愚直に調べる．画像サイズ$N$を変化させるごとに，猫写像を何度も作用させ，画像が一致するまで繰り返す．下図の横軸が$N$，縦軸が周期である．傾向として，$N$が大きくなるに従って周期も大きくなる．また，なんとなく規則性もありそう...
J. Bao and Q. Yang. Period of the discrete Arnold cat map and general cat map, Nonlinear Dynamics, 70(2012), pp.</description>
    </item>
    
    <item>
      <title>中の人について</title>
      <link>https://kitano-AppMath.github.io/kitano_blog/aboutme/</link>
      <pubDate>Fri, 10 Feb 2023 21:14:23 +0900</pubDate>
      
      <guid>https://kitano-AppMath.github.io/kitano_blog/aboutme/</guid>
      <description>初めまして，kitanoと申します．ここは簡単な自己紹介用のページです．
目次 ブログ運営目的と方針 興味の対象 ブログ運営目的と方針 以前はドメインを取得して拙いブログを公開しておりましたが，Hugoを使って作り直すことにしました．以前書いた記事も修正して掲載し直す予定です．
各記事は，基本的に，簡単な解説と実験を掲載するだけです．プログラミング言語としてはJulia言語を使っています．
興味の対象 興味の対象，あるいはこれから勉強したいと思っている分野は以下の通りです．
関数解析 確率論 数理統計学 最適化 機械学習 自然言語 複雑系 力学系 カオス 複雑ネットワーク 信号処理 数値解析・数値計算 Bayes統計・Bayes推定 コンピュータサイエンス 言語学 音声学 コーパス言語学 Julia言語 あと，中高生の頃からサメとクラシック音楽が好きです．</description>
    </item>
    
    <item>
      <title>変分推論法</title>
      <link>https://kitano-AppMath.github.io/kitano_blog/posts/2023-02-11/</link>
      <pubDate>Fri, 10 Feb 2023 16:44:19 +0900</pubDate>
      
      <guid>https://kitano-AppMath.github.io/kitano_blog/posts/2023-02-11/</guid>
      <description>目次 変分推論法の概要 導入 問題設定 基本的な考え方 経験Bayes法との組み合わせ 平均場近似による方法 方針 例１：単純な例 勾配計算による方法 方針 例２：ロジスティック回帰 変分推論法の概要 ここでは，変分推論法の基本となる考え方を紹介する．
導入 変分推論法とは，主にBayes推定において，複雑な事後分布を，より単純な分布で近似する最適化ベースの学習方法 を指す．具体的にどのような分布で近似するかは後に回して，ここでは，どのように近似するかを述べ，最適化問題として定式化しておこう．変分推論法の包括的な説明としては，Bealの博士論文1やBleiのレビュー論文2が参考になりそう．この記事の実験で使ったJuliaプログラムはこちら．
問題設定 いま，手元には$N$次元データからなる有限集合$\mathcal{D}$があるとしよう．統計家は，データ点$x_1, \dots, x_{\#\mathcal{D}}\in\mathcal{D}$を，未知の真の分布$\mathcal{Q}$に従う独立な確率変数列$X_1, \dots, X_{\#\mathcal{D}}$の一組の実現値であると想定し，真の分布を推測するための確率モデル$\mathcal{P}_\theta$を作る．そして，予測分布$\mathcal{P}^* $を計算し，真の分布$\mathcal{Q}$はおおよそ${\mathcal{P}}^* $であろうと推測する．ただし，$\theta\in\Theta$は未知のパラメータであり，データと確率モデルから推定する必要がある．Bayes推定を用いる場合には，確率モデル$p(\cdot\mid\theta)$と，既知のパラメータ$\lambda\in\Lambda$を持つ事前分布$\pi_{\lambda}$を作り，次式で定義される事後分布を計算する： \begin{equation} \pi^*_{\lambda} (\theta\mid\mathcal{D}) = \frac{1}{\mathcal{M}_\lambda}\left[\prod_{x\in\mathcal{D}}p(x\mid\theta)\right]\pi_{\lambda}(\theta). \end{equation} ただし，$\mathcal{M}_{\lambda}$は周辺尤度と呼ばれ， \begin{equation} \mathcal{M}_{\lambda} = \int \left[\prod_{x\in\mathcal{D}}p(x\mid\theta)\right]\pi_{\lambda}(\theta)\mathrm{d}{\theta} \end{equation} で定義される．この事後分布に対して，（Bayes事後）予測分布を \begin{equation} {p_{\lambda}}^* (x) = \int p(x\mid\theta)\pi_{\lambda}^*(\theta\mid\mathcal{D})\mathrm{d}\theta \end{equation} と定義する．この記事では，事後分布の解析的な計算が難しい場合を考えよう．
基本的な考え方 さて，冒頭で述べたように，変分推論法では事後分布を近似する．近似事後分布の族を$\mathcal{R}$とし，近似事後分布と事後分布との近さをKullback-Leiblerダイバージェンス \begin{equation} D_{\mathrm{KL}}[r \Vert \pi_{\lambda}^* ] = \int\log\left[\frac{r(\theta\mid\mathcal{D})}{\pi_{\lambda}^* (\theta\mid\mathcal{D})}\right]r(\theta\mid\mathcal{D})\mathrm{d}\theta, \quad r(\theta\mid\mathcal{D})\in\mathcal{R} \end{equation} で測ろう．これを最小化できると良いのだが，事後分布に周辺尤度の計算を含むため，次のような量を定義する： \begin{equation} \mathcal{L}_{\lambda}[r] = -D_{\mathrm{KL}}[r \Vert \pi_{\lambda}^* ] + \log \mathcal{M}_{\lambda}.</description>
    </item>
    
  </channel>
</rss>
